base_model: Gryphe/MythoMax-L2-13b
#base_model: TheBloke/MythoMax-L2-13B-GPTQ
model_type: AutoModelForCausalLM
tokenizer_type: AutoTokenizer

load_in_8bit: false
load_in_4bit: true
use_nested_quant: true
double_quant: true
strict: false

device_map: auto

datasets:
  - path: lora_training_data.jsonl
    type: alpaca
  - path: lora_eval_data.jsonl
    type: alpaca

dataset_prepared_path: last_run_prepared
val_set_size: 0.05

adapter: lora
output_dir: ./lora-output

sequence_len: 2048
sample_packing: true
pad_to_sequence_len: true

lora_r: 8
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules:
  - q_proj
  - k_proj
  - v_proj
  - o_proj

train_batch_size: 2
micro_batch_size: 1
num_epochs: 3
max_steps: -1
learning_rate: 2e-4
lr_scheduler: cosine
warmup_steps: 100
weight_decay: 0.05
optimizer: adamw_torch
hard_clip_gradient: 1.0

gradient_checkpointing: true
gradient_accumulation_steps: 4

save_steps: 100
eval_steps: 100
logging_steps: 10

flash_attention: true
mixed_precision: bf16
trust_remote_code: true